{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1035e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import gc\n",
    "sys.path.append('./tvae')\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c798a5b",
   "metadata": {},
   "source": [
    "# Load videos and stack flies to perform mean-centering and SVD\n",
    "The TVAE accepts a sequence of frames as inputs. Before we split the videos into sequences, we are going to preprocess all the poses by normalizing the keypoint coordinates, mean-centering them, rotationally aligning the spine with the y-axis, and computing the singular value decomposition of the aligned pose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a425e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train = np.load('fly_data/user_train.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973fd405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                                    | 5/426 [00:00<00:42,  9.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# First we need to combine all the videos and normalize them to compute SVD\n",
    "from datasets.fly_v1.preprocess import normalize\n",
    "\n",
    "full_dataset = []\n",
    "for i, (name, sequence) in enumerate(tqdm(user_train['sequences'].items())):\n",
    "    # Vectorizes each frame in the sequence - only indexing the 19 keypoints for alignment / SVD\n",
    "    vec_seq = (sequence['keypoints'][:,:,:19]).reshape(sequence['keypoints'].shape[0], -1)\n",
    "    vec_seq = np.nan_to_num(vec_seq)\n",
    "    full_dataset.append(normalize(vec_seq))\n",
    "    \n",
    "full_dataset = np.concatenate(full_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f8859",
   "metadata": {},
   "source": [
    "The output of the last step is an array of each of the frames stacked on top of one another. Each of the 11 flies has 19 keypoints, with both x and y coordinates, plus 10 additional features. So `full_dataset` will be `total_frames x [(19 * 2 + 10) * 11 = 528]`. \n",
    "\n",
    "The next step is to rotationally align each of the fly poses such that the vector formed by the keypoint at the neck base and the keypoint at the base of the body is aligned with the y-axis. The angle of rotation is stored and fed as an input to the model. After the poses are rotationally aligned, the mean of each fly's pose is subtracted, and the singular value decomposition of the normalized, aligned, and mean-centered pose is computed. In a sense, we are trying to limit the relevant sources of variance that the TVAE representations will capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to complete the rotation alignment and compute SVD\n",
    "from datasets.fly_v1.preprocess import transform_to_svd_components\n",
    "_, svd, mean = transform_to_svd_components(full_dataset)\n",
    "del full_dataset, _\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a736754",
   "metadata": {},
   "source": [
    "# Create sub-sequences with sliding window and complete preprocessing\n",
    "Next we iterate through each video and apply the preprocessing stack we developed in the last step. After each video is preprocessed, we pad the beginning and end of the video with repeat frames and create fixed-length sliding window sequences defined by `sub_seq_length` and `sliding_window` below. These sequences will be the inputs provided as inputs to the TVAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf64a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we need to iterate through the videos, rotationally align, mean subtract, and apply SVD\n",
    "sub_seq_length = 21\n",
    "sliding_window = 5\n",
    "preprocessed_data = []\n",
    "for i, (name, sequence) in enumerate(tqdm(user_train['sequences'].items())):\n",
    "    # Preprocess sequences\n",
    "    vec_seq = sequence['keypoints'].reshape(sequence['keypoints'].shape[0], -1)\n",
    "    vec_seq = normalize(vec_seq)\n",
    "    \n",
    "    if i == 0: # Here for debugging - leave to demonstrate that preprocessing works\n",
    "        control = vec_seq\n",
    "        control = control.reshape(control.shape[0], -1)\n",
    "        control = np.pad(control, ((sub_seq_length//2, sub_seq_length-1-sub_seq_length//2), (0,0)), mode='edge')\n",
    "        control = np.stack([control[i:len(control)+i-sub_seq_length+1:sliding_window] for i in range(sub_seq_length)], axis=1)\n",
    "\n",
    "    # Apply preprocessing / rotational alignment only to keypoints\n",
    "    vec_seq = np.nan_to_num(vec_seq)\n",
    "    flies = np.array_split(vec_seq, 11, axis=1)\n",
    "    fly_kps = np.concatenate([fly[:,:38] for fly in flies], axis=1)\n",
    "    \n",
    "    processed_kps, _, _ = transform_to_svd_components(\n",
    "            fly_kps,\n",
    "            svd_computer=svd,\n",
    "            mean=mean\n",
    "    )\n",
    "\n",
    "    # Join keypoints and additional features\n",
    "    fly_feats = np.stack([fly[:,38:] for fly in flies], axis=1)\n",
    "    # Output after this step is [num frames x num flies x (processed keypoints + feats)] = [4500 x 11 x (14 + 10)]\n",
    "    fly_data = np.concatenate((processed_kps, fly_feats), axis=2) \n",
    "      \n",
    "    # Pads the beginning and end of the sequence with duplicate frames\n",
    "    fly_data = fly_data.reshape(fly_data.shape[0], -1)\n",
    "    pad_vec = np.pad(fly_data, ((sub_seq_length//2, sub_seq_length-1-sub_seq_length//2), (0,0)), mode='edge')\n",
    "    \n",
    "    # Converts sequence into [number of sub-sequences, frames in sub-sequence, x/y alternating keypoints]\n",
    "    sub_seqs = np.stack([pad_vec[i:len(pad_vec)+i-sub_seq_length+1:sliding_window] for i in range(sub_seq_length)], axis=1)\n",
    "      \n",
    "    preprocessed_data.append(sub_seqs)\n",
    "        \n",
    "preprocessed_data = np.concatenate(preprocessed_data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acf0db",
   "metadata": {},
   "source": [
    "# Postprocessing demonstration\n",
    "This next cell demonstrates how to postprocess the pose and also serves as a sanity check to make sure the unalignment, SVD, and unnormalization give us back approximately the same pose we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea84261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test postprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from datasets.fly_v1.preprocess import transform_svd_to_keypoints\n",
    "from datasets.fly_v1.preprocess import unnormalize\n",
    "\n",
    "ARENA_RADIUS_MM = 26.689\n",
    "\n",
    "random_seq = preprocessed_data[100] # Cannot compare postprocessed/original pairs outside of first sequence (index 1800)\n",
    "random_seq = np.array_split(random_seq, 11, axis=1)\n",
    "random_seq = np.stack([elem[:,:14] for elem in random_seq], axis=1) # Extract only the centers + rotations + SVD\n",
    "random_seq = transform_svd_to_keypoints(random_seq, svd, mean)\n",
    "random_seq = unnormalize(random_seq.reshape(random_seq.shape[0],-1))\n",
    "\n",
    "control_seq = control[100] \n",
    "control_seq = np.array_split(control_seq, 11, axis=1)\n",
    "control_seq = np.concatenate([seq[:,:38] for seq in control_seq], axis=1)\n",
    "control_seq = unnormalize(control_seq)\n",
    "\n",
    "from celluloid import Camera\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)\n",
    "minv = -ARENA_RADIUS_MM*1.01\n",
    "maxv = ARENA_RADIUS_MM*1.01\n",
    "alpha=0.5\n",
    "plt.xlim(minv,maxv), plt.ylim(minv,maxv)\n",
    "for i in range(0,21):\n",
    "    xvals = random_seq[i,::2]\n",
    "    yvals = random_seq[i,1::2]\n",
    "    if i == 0:\n",
    "        plt.scatter(x=xvals, y=yvals, label='After postprocessing', c='r')\n",
    "    else:\n",
    "        plt.scatter(x=xvals, y=yvals,  color='r')\n",
    "        \n",
    "    xvals = control_seq[i,::2]\n",
    "    yvals = control_seq[i,1::2]\n",
    "    if i == 0:  \n",
    "        plt.scatter(x=xvals, y=yvals, label = 'Before preprocessing', color=[alpha, alpha, 1.])\n",
    "    else:\n",
    "        plt.scatter(x=xvals, y=yvals, color=[alpha, alpha, 1.])\n",
    "        \n",
    "    camera.snap()\n",
    "    \n",
    "plt.legend()\n",
    "animation = camera.animate()\n",
    "animation.save('test.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17f52a",
   "metadata": {},
   "source": [
    "# Configure the model and training parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "660ee6fe",
   "metadata": {},
   "source": [
    "<!-- ![Screenshot%20from%202022-03-16%2016-02-04.png]() -->\n",
    "<div>\n",
    "<img src=\"tvae_diagram.png\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "Above is the architecture that defines the trajectory variational autoencoder (TVAE) used here. The input to the model is comprised of states $s_{0}, ... , s_{\\tau}$ and actions that define the change from one state to the next $a_{0}, ... , a_{\\tau - 1}$. The encoder is a bi-directional GRU, the output of which, at each time step, is averaged and fed into fully-connected layers that produce a mean and variance which define a gaussian distribution (the posterior). A sample is then generated from the posterior and passed as an input, along with the initial state $s_{0}$ to the decoder. \n",
    "\n",
    "The decoder then \"rolls out\" each subsequent state by predicting a distribution of actions, and then sampling a single action used to act on the previous state i.e. $\\hat{s_1} = s_{0} + \\hat{a_{0}}, \\hat{s_2} = \\hat{s_1} + \\hat{a_1}, \\hat{s_3} = \\hat{s_2} + \\hat{a_2}$ etc. The output of each recurrent unit in the decoder is fed through fully-connected layers to produce the mean and variance of the gaussian distribution of actions. The reconstruction loss for a single trajectory is computed by taking the negative log-likelihood of the true action under the predicted distribution of actions and summing that across time. For more detail, the main computation of a forward pass through the model is completed in `./tvae/lib/models/core.py` in the method titled `forward`.\n",
    "\n",
    "\n",
    "In this application, the input trajectory will be a sequence of preprocessed poses, and we will treat the mean of the posterior as our embedding. Below are adjustable model and dataset configurations and their associated descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model arguments\n",
    "model_config = { \n",
    "    \"name\": \"tvae\",\n",
    "    \"z_dim\": 11,      # Dimensionality of the posterior / embedding\n",
    "    \"h_dim\": 256,     # Number of units to use in the fully-connected layers in the encoder and decoder\n",
    "    \"rnn_dim\": 256,   # Number of units to use in the recurrent encoder and decoder\n",
    "    \"num_layers\": 1,  # Number of layers to use in the recurrent encoder and decoder \n",
    "    \"state_dim\": 24,  # Dimensionality of the input states - we are only embedding single fly features for now\n",
    "    \"action_dim\": 24  # Dimensionality of the input actions\n",
    "}\n",
    "\n",
    "# Example training arguments\n",
    "train_config = { \n",
    "    \"batch_size\": 512,          # Batch size to use during training - higher = faster, but more GPU utilization\n",
    "    \"num_epochs_til_val\": 10,   # Checkpoints will be saved to ./checkpoints/{project_name}/ every num_epochs_til_val epochs\n",
    "    \"learning_rate\": 0.0002,    # Learning rate for the optimizer to use\n",
    "    \"num_epochs\": 300,          # Number of epochs to use\n",
    "    \"clip\": 10,                 # Higher means potentially faster but more unstable training\n",
    "    \"project_name\": 'test_separate_flies_jupyter_notebook'    # All checkpoints and loss logs will be saved to ./checkpoints/project_name/\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8b20f",
   "metadata": {},
   "source": [
    "# Split preprocessed data into training a validation sets\n",
    "We are only going to embed one fly at a time, and then concatenate the embeddings produced for each fly and treat the result as our embedding for the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only embedding one fly at a time and then concatenating embeddings\n",
    "preprocessed_data = np.concatenate([preprocessed_data[:,:,i*24:(i+1)*24] for i in range(11)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0411785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily use a smaller dataset size for training purposes\n",
    "np.random.shuffle(preprocessed_data)\n",
    "preprocessed_data = preprocessed_data[:2000000]\n",
    "\n",
    "# Split the dataset into a training and testing set\n",
    "test_prop = 0.20\n",
    "test_len = int(len(preprocessed_data) * test_prop)\n",
    "\n",
    "data_train = preprocessed_data[test_len:]\n",
    "data_test = preprocessed_data[:test_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f515b7b",
   "metadata": {},
   "source": [
    "# Create dataset object to train the TVAE using preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Device configurations\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "data_config = { \n",
    "    'name': 'fly_v1',\n",
    "    'data_train': data_train,\n",
    "    'data_test': data_test,\n",
    "    'device': device,\n",
    "}   \n",
    "\n",
    "# build dataset and train / val split\n",
    "dataset = load_dataset(data_config)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=train_config['batch_size'],\n",
    "                shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710a784",
   "metadata": {},
   "source": [
    "# Instantiate model and load most recent checkpoint if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will check and store the most recent checkpointed model in your project directory\n",
    "from util.checkpoint_handler import checkpoint_handler\n",
    "train_config = checkpoint_handler(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fab6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = TVAE(model_config)\n",
    "model = model.to(device)\n",
    "model.prepare_stage(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb05a65",
   "metadata": {},
   "source": [
    "# Train the TVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses, val_losses = train(model, data_loader, train_config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51161c6",
   "metadata": {},
   "source": [
    "# Pick best checkpoint\n",
    "\n",
    "We will pick the checkpoint with the best reconstruction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./checkpoints/{train_config[\"project_name\"]}/val_losses.txt', 'r') as myFile:\n",
    "    lines = myFile.readlines()\n",
    "    klds, nlls = [],[]\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'NLL' in line:\n",
    "            nlls.append(float(line.split(' ')[-1]))\n",
    "        elif 'KLD' in line:\n",
    "            klds.append(float(line.split(' ')[-1]))\n",
    "\n",
    "best_ckpt = np.array(nlls).argmin()*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6da87c",
   "metadata": {},
   "source": [
    "# Visualize some reconstructions of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e793b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reconstruct import *\n",
    "\n",
    "best_ckpt = 0\n",
    "model.load_state_dict(torch.load(f'./checkpoints/{train_config[\"project_name\"]}/{best_ckpt}.pt'))\n",
    "\n",
    "data_loader.dataset.eval()\n",
    "model.eval()\n",
    "recons, originals, embeddings = reconstruct(model, data_loader, data_config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eca75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_seq = originals[100] # Cannot compare postprocessed/original pairs outside of first sequence (index 1800)\n",
    "original_seq = np.expand_dims(original_seq[:,:14], axis=1)\n",
    "original_seq = transform_svd_to_keypoints(original_seq, svd, mean, single=True)\n",
    "original_seq = unnormalize(original_seq.reshape(original_seq.shape[0],-1))\n",
    "\n",
    "recon_seq = recons[100] # Cannot compare postprocessed/original pairs outside of first sequence (index 1800)\n",
    "recon_seq = np.expand_dims(recon_seq[:,:14], axis=1)\n",
    "recon_seq = transform_svd_to_keypoints(recon_seq, svd, mean, single=True)\n",
    "recon_seq = unnormalize(recon_seq.reshape(recon_seq.shape[0],-1))\n",
    "\n",
    "from celluloid import Camera\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)\n",
    "minv = -ARENA_RADIUS_MM*1.01\n",
    "maxv = ARENA_RADIUS_MM*1.01\n",
    "alpha=0.5\n",
    "plt.xlim(minv,maxv), plt.ylim(minv,maxv)\n",
    "for i in range(0,21):\n",
    "    xvals = recon_seq[i,::2]\n",
    "    yvals = recon_seq[i,1::2]\n",
    "    if i == 0:\n",
    "        plt.scatter(x=xvals, y=yvals, label='Reconstructed', c='r')\n",
    "    else:\n",
    "        plt.scatter(x=xvals, y=yvals,  color='r')\n",
    "        \n",
    "    xvals = original_seq[i,::2]\n",
    "    yvals = original_seq[i,1::2]\n",
    "    if i == 0:  \n",
    "        plt.scatter(x=xvals, y=yvals, label = 'Original', color=[alpha, alpha, 1.])\n",
    "    else:\n",
    "        plt.scatter(x=xvals, y=yvals, color=[alpha, alpha, 1.])\n",
    "        \n",
    "    camera.snap()\n",
    "    \n",
    "plt.legend()\n",
    "animation = camera.animate()\n",
    "animation.save('test.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5a222",
   "metadata": {},
   "source": [
    "# Example submission\n",
    "This just iterates through all the sequences provided in sample clips, preprocesses them, produces embeddings for them, and then saves them in the correct dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reconstruct import *\n",
    "\n",
    "submission_clips = np.load('fly_data/submission_data.npy',allow_pickle=True).item()\n",
    "model.load_state_dict(torch.load(f'./checkpoints/{train_config[\"project_name\"]}/{best_ckpt}.pt'))\n",
    "model.eval()\n",
    "submission = {\n",
    "    'frame_number_map': {},\n",
    "    'embeddings': []\n",
    "}\n",
    "\n",
    "# Now we need to iterate through the videos, rotationally align, mean subtract, and apply SVD\n",
    "start_idx = 0\n",
    "for i, (name, sequence) in enumerate(tqdm(submission_clips['sequences'].items())):\n",
    "    # Preprocess sequences\n",
    "    vec_seq = sequence['keypoints'].reshape(sequence['keypoints'].shape[0], -1)\n",
    "    vec_seq = normalize(vec_seq)\n",
    "    \n",
    "    # Apply preprocessing / rotational alignment only to keypoints\n",
    "    vec_seq = np.nan_to_num(vec_seq)\n",
    "    flies = np.array_split(vec_seq, 11, axis=1)\n",
    "    fly_kps = np.concatenate([fly[:,:38] for fly in flies], axis=1)\n",
    "    \n",
    "    processed_kps, _, _ = transform_to_svd_components(\n",
    "            fly_kps,\n",
    "            svd_computer=svd,\n",
    "            mean=mean\n",
    "    )\n",
    "\n",
    "    # Join keypoints and additional features\n",
    "    fly_feats = np.stack([fly[:,38:] for fly in flies], axis=1)\n",
    "    # Output after this step is [num frames x num flies x (processed keypoints + feats)] = [4500 x 11 x (14 + 10)]\n",
    "    fly_data = np.concatenate((processed_kps, fly_feats), axis=2) \n",
    "      \n",
    "    # Pads the beginning and end of the sequence with duplicate frames\n",
    "    fly_data = fly_data.reshape(fly_data.shape[0], -1)\n",
    "    pad_vec = np.pad(fly_data, ((sub_seq_length//2, sub_seq_length-1-sub_seq_length//2), (0,0)), mode='edge')\n",
    "    \n",
    "    # Converts sequence into [number of sub-sequences, frames in sub-sequence, x/y alternating keypoints]\n",
    "    data_test = np.stack([pad_vec[i:len(pad_vec)+i-sub_seq_length+1:sliding_window] for i in range(sub_seq_length)], axis=1)\n",
    "    \n",
    "    # Only embedding one fly at a time and then concatenating embeddings\n",
    "    flies = [data_test[:,:,i*24:(i+1)*24] for i in range(11)]\n",
    "    \n",
    "    # Device configurations\n",
    "    device = torch.device('cuda:1')\n",
    "\n",
    "    fly_embeddings = []\n",
    "    for fly in flies:\n",
    "        data_config = { \n",
    "            'name': 'fly_v1',\n",
    "            'data_train': fly,\n",
    "            'data_test': fly,\n",
    "            'device': device,\n",
    "        }   \n",
    "\n",
    "        # build dataset and train / val split\n",
    "        dataset = load_dataset(data_config)\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=train_config['batch_size'],\n",
    "                        shuffle = False\n",
    "        )\n",
    "\n",
    "        data_loader.dataset.eval()\n",
    "        \n",
    "        _, _, embeddings = reconstruct(model, data_loader, data_config['device'])\n",
    "        fly_embeddings.append(embeddings)\n",
    "    \n",
    "    embeddings = np.concatenate(fly_embeddings, axis=1)\n",
    "    \n",
    "    end_idx = start_idx + len(embeddings)\n",
    "    submission['embeddings'].append(embeddings)\n",
    "    submission['frame_number_map'][name] = (start_idx, end_idx)\n",
    "    start_idx = end_idx\n",
    "    \n",
    "submission['embeddings'] = np.concatenate(submission['embeddings'], axis=0)\n",
    "np.save('test_fly_tvae_submission.npy', submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
